nohup: ignoring input
Dataset: breakfast	Split: 4
Batch Size: 1	Num in channels: 2048	Num Workers: 4

------------------------Loading Model------------------------

Multi Stage TCN will be used as a model.
stages: ['dilated', 'dilated', 'dilated', 'dilated']	n_features: 64	n_layers of dilated TCN: 10	kernel_size of ED-TCN: 15
Adam will be used as an optimizer.

---------------------------Start training---------------------------

epoch: 0  lr: 0.0005  train_time: 743.5s  val_time: 274.0s  train loss: 12.9684  val loss: 3.0553  val_acc: 12.9728  val_edit: 24.0774 F1s: [23.044870627546807, 19.16438020973962, 11.498626132844878]
epoch: 1  lr: 0.0005  train_time: 492.4s  val_time: 223.9s  train loss: 10.2063  val loss: 2.5921  val_acc: 25.9192  val_edit: 37.1840 F1s: [36.68600413992059, 28.497738504846637, 15.43089990609385]
epoch: 2  lr: 0.0005  train_time: 321.3s  val_time: 181.7s  train loss: 8.6376  val loss: 2.4268  val_acc: 24.3873  val_edit: 37.9163 F1s: [37.09098324997627, 29.916290348574094, 17.088809100612963]
epoch: 3  lr: 0.0005  train_time: 300.1s  val_time: 177.8s  train loss: 7.6544  val loss: 2.2371  val_acc: 25.0952  val_edit: 38.3875 F1s: [37.694265505562996, 31.10646685095863, 19.461836834721574]
epoch: 4  lr: 0.0005  train_time: 308.2s  val_time: 173.5s  train loss: 7.0807  val loss: 1.9478  val_acc: 27.6812  val_edit: 46.5725 F1s: [42.66390843702313, 35.16778040398247, 21.74496161203657]
epoch: 5  lr: 0.0005  train_time: 304.1s  val_time: 174.6s  train loss: 6.3587  val loss: 1.7621  val_acc: 33.7822  val_edit: 51.5481 F1s: [46.27458702887798, 39.3470022653787, 26.161569029715544]
epoch: 6  lr: 0.0005  train_time: 306.8s  val_time: 172.6s  train loss: 5.8802  val loss: 1.6532  val_acc: 39.5606  val_edit: 53.6035 F1s: [49.32935429121387, 41.984240388723, 29.2739998096302]
epoch: 7  lr: 0.0005  train_time: 306.3s  val_time: 179.0s  train loss: 5.4845  val loss: 1.6992  val_acc: 43.3104  val_edit: 51.1298 F1s: [47.89300011849761, 40.46279101118425, 26.663831240459583]
epoch: 8  lr: 0.0005  train_time: 304.9s  val_time: 175.3s  train loss: 5.2068  val loss: 2.1905  val_acc: 39.9075  val_edit: 50.3942 F1s: [48.484231257567444, 40.905411532426065, 27.76879067551445]
epoch: 9  lr: 0.0005  train_time: 303.6s  val_time: 176.5s  train loss: 4.9129  val loss: 1.5165  val_acc: 47.1391  val_edit: 51.2782 F1s: [50.87699694852571, 43.86098213209698, 31.28880651383813]
epoch: 10  lr: 0.0005  train_time: 309.1s  val_time: 176.3s  train loss: 4.8017  val loss: 1.7481  val_acc: 42.3203  val_edit: 49.0972 F1s: [48.58357926462716, 40.69730760332189, 28.245299717050475]
epoch: 11  lr: 0.0005  train_time: 300.9s  val_time: 179.6s  train loss: 4.4467  val loss: 1.7224  val_acc: 40.3066  val_edit: 53.9248 F1s: [51.28369905631991, 44.08636314483648, 31.689757709236726]
epoch: 12  lr: 0.0005  train_time: 299.6s  val_time: 180.3s  train loss: 4.4850  val loss: 2.0633  val_acc: 41.6081  val_edit: 52.9446 F1s: [48.59376105955091, 41.506747149509366, 29.05878288917861]
epoch: 13  lr: 0.0005  train_time: 308.3s  val_time: 177.2s  train loss: 4.0865  val loss: 2.0324  val_acc: 34.2727  val_edit: 47.3887 F1s: [43.932157274495694, 37.57489110205748, 26.18055780257567]
epoch: 14  lr: 0.0005  train_time: 311.1s  val_time: 179.7s  train loss: 4.0051  val loss: 1.5607  val_acc: 45.9736  val_edit: 57.0539 F1s: [53.56052948526708, 46.765536322334576, 33.743552520925284]
epoch: 15  lr: 0.0005  train_time: 302.8s  val_time: 177.0s  train loss: 4.2048  val loss: 1.9211  val_acc: 47.6733  val_edit: 58.9421 F1s: [57.100989393132195, 49.042381334524194, 34.536886829029896]
epoch: 16  lr: 0.0005  train_time: 304.4s  val_time: 179.0s  train loss: 3.5824  val loss: 2.0031  val_acc: 48.0278  val_edit: 55.6207 F1s: [52.08310416487558, 46.09752982964042, 33.695764300520146]
epoch: 17  lr: 0.0005  train_time: 305.0s  val_time: 170.6s  train loss: 3.8141  val loss: 1.6834  val_acc: 48.6144  val_edit: 59.8353 F1s: [57.81249510133384, 50.499127045778344, 36.69704371244521]
epoch: 18  lr: 0.0005  train_time: 303.1s  val_time: 174.9s  train loss: 3.5857  val loss: 1.7844  val_acc: 51.8066  val_edit: 59.2872 F1s: [58.68691859550415, 51.41616439040383, 37.76450947993195]
epoch: 19  lr: 0.0005  train_time: 309.4s  val_time: 174.4s  train loss: 3.8177  val loss: 1.6872  val_acc: 48.4287  val_edit: 52.6921 F1s: [52.55413463124985, 44.907258659848, 31.644384346305575]
epoch: 20  lr: 0.0005  train_time: 302.8s  val_time: 171.0s  train loss: 3.1398  val loss: 1.8348  val_acc: 46.7147  val_edit: 51.6826 F1s: [50.09016167341282, 42.707112983479746, 30.847560633845927]
epoch: 21  lr: 0.0005  train_time: 309.2s  val_time: 170.7s  train loss: 3.1684  val loss: 1.9260  val_acc: 51.7363  val_edit: 58.8106 F1s: [57.2557945962899, 50.161235145546165, 36.42982975700999]
epoch: 22  lr: 0.0005  train_time: 306.1s  val_time: 177.6s  train loss: 3.5880  val loss: 1.6498  val_acc: 52.5786  val_edit: 55.4300 F1s: [56.363822912317666, 48.59458404506854, 35.645852599653416]
epoch: 23  lr: 0.0005  train_time: 308.7s  val_time: 166.3s  train loss: 2.8395  val loss: 1.9424  val_acc: 46.2704  val_edit: 58.3966 F1s: [54.276103569916145, 47.699091592610046, 35.23849062602059]
epoch: 24  lr: 0.0005  train_time: 301.8s  val_time: 171.0s  train loss: 3.0722  val loss: 1.8742  val_acc: 50.5325  val_edit: 59.0942 F1s: [57.79060574960626, 50.775313267518825, 36.68056404283717]
epoch: 25  lr: 0.0005  train_time: 301.6s  val_time: 171.7s  train loss: 3.3706  val loss: 1.7531  val_acc: 50.9717  val_edit: 54.5636 F1s: [54.94000558675617, 47.90818503380891, 35.57641664570267]
epoch: 26  lr: 0.0005  train_time: 304.7s  val_time: 173.1s  train loss: 2.8284  val loss: 1.8416  val_acc: 56.1837  val_edit: 60.3769 F1s: [59.26239445354577, 52.94615876045552, 40.313687374275084]
epoch: 27  lr: 0.0005  train_time: 301.7s  val_time: 169.1s  train loss: 3.0944  val loss: 2.3720  val_acc: 47.8176  val_edit: 52.6680 F1s: [52.68859874567519, 45.746383657392755, 31.82182506991304]
epoch: 28  lr: 0.0005  train_time: 307.9s  val_time: 174.4s  train loss: 2.7074  val loss: 1.9121  val_acc: 53.6696  val_edit: 58.3807 F1s: [56.73204817092784, 49.80999825822379, 36.02751905518401]
epoch: 29  lr: 0.0005  train_time: 310.3s  val_time: 170.0s  train loss: 3.0370  val loss: 2.1748  val_acc: 47.9437  val_edit: 51.2712 F1s: [51.041093972830765, 44.369339912000136, 32.159593918323246]
epoch: 30  lr: 0.0005  train_time: 303.3s  val_time: 173.7s  train loss: 2.5635  val loss: 2.0620  val_acc: 52.8113  val_edit: 61.8790 F1s: [61.14204116877686, 53.783159194762284, 39.41477836716118]
epoch: 31  lr: 0.0005  train_time: 308.1s  val_time: 171.8s  train loss: 2.6236  val loss: 2.0500  val_acc: 48.7280  val_edit: 57.1370 F1s: [55.77182819552711, 48.173202193653545, 34.516493543710986]
epoch: 32  lr: 0.0005  train_time: 300.2s  val_time: 173.6s  train loss: 2.7227  val loss: 1.8648  val_acc: 58.1142  val_edit: 60.3281 F1s: [59.828679471044246, 53.15176315213369, 39.09509721758521]
epoch: 33  lr: 0.0005  train_time: 310.1s  val_time: 175.7s  train loss: 2.6269  val loss: 2.4424  val_acc: 47.1830  val_edit: 54.4909 F1s: [52.676111064166534, 45.94879987313636, 32.47302242443761]
epoch: 34  lr: 0.0005  train_time: 310.6s  val_time: 170.9s  train loss: 2.8086  val loss: 2.0172  val_acc: 58.6938  val_edit: 60.1077 F1s: [58.74345064591559, 51.79057106476381, 38.03140876109905]
epoch: 35  lr: 0.0005  train_time: 307.6s  val_time: 175.6s  train loss: 2.4615  val loss: 2.0749  val_acc: 60.8264  val_edit: 61.6184 F1s: [60.300685970299185, 53.79926378419599, 39.150746921132374]
epoch: 36  lr: 0.0005  train_time: 316.1s  val_time: 172.7s  train loss: 2.7208  val loss: 2.1683  val_acc: 59.1095  val_edit: 58.1815 F1s: [53.59001587675878, 47.34308531983909, 34.47597992744167]
epoch: 37  lr: 0.0005  train_time: 302.4s  val_time: 170.7s  train loss: 2.3812  val loss: 2.1266  val_acc: 53.7405  val_edit: 61.1966 F1s: [59.22278764834734, 52.45626413310699, 39.32124789764051]
epoch: 38  lr: 0.0005  train_time: 305.8s  val_time: 170.9s  train loss: 2.9406  val loss: 1.9051  val_acc: 58.6016  val_edit: 58.9664 F1s: [55.491435949315374, 49.68262656289496, 36.66089161386646]
epoch: 39  lr: 0.0005  train_time: 300.9s  val_time: 174.2s  train loss: 2.1968  val loss: 2.0043  val_acc: 59.9819  val_edit: 59.1594 F1s: [55.624032265165944, 49.03697525438019, 35.93990283527404]
epoch: 40  lr: 0.0005  train_time: 303.2s  val_time: 174.0s  train loss: 2.4939  val loss: 1.9529  val_acc: 64.6516  val_edit: 63.5871 F1s: [62.007309294151256, 55.54652099671124, 42.62494440183123]
epoch: 41  lr: 0.0005  train_time: 308.2s  val_time: 173.5s  train loss: 2.4898  val loss: 2.0910  val_acc: 47.6147  val_edit: 57.1907 F1s: [51.77692771128477, 45.07747517347283, 33.0147489973572]
epoch: 42  lr: 0.0005  train_time: 302.7s  val_time: 169.8s  train loss: 2.1956  val loss: 2.0292  val_acc: 63.3416  val_edit: 63.5028 F1s: [61.04838230327822, 54.83870488392342, 40.82660810973002]
epoch: 43  lr: 0.0005  train_time: 309.8s  val_time: 171.5s  train loss: 2.6373  val loss: 1.9468  val_acc: 61.8709  val_edit: 61.6038 F1s: [56.765232543602465, 51.11723406494889, 39.0415470819691]
epoch: 44  lr: 0.0005  train_time: 306.2s  val_time: 177.5s  train loss: 2.0109  val loss: 2.0392  val_acc: 63.1762  val_edit: 63.5819 F1s: [61.277113479883674, 55.23945486342854, 41.36307336527067]
epoch: 45  lr: 0.0005  train_time: 309.4s  val_time: 171.6s  train loss: 2.1529  val loss: 2.1384  val_acc: 60.0104  val_edit: 58.5497 F1s: [58.224775573318674, 51.23699409295924, 38.129854842227296]
epoch: 46  lr: 0.0005  train_time: 306.1s  val_time: 171.0s  train loss: 2.5143  val loss: 1.9264  val_acc: 58.8679  val_edit: 60.9983 F1s: [57.322529845005, 50.519290207082236, 37.1224345615369]
epoch: 47  lr: 0.0005  train_time: 311.1s  val_time: 173.7s  train loss: 1.9061  val loss: 2.1812  val_acc: 60.2461  val_edit: 64.8336 F1s: [62.51579792123428, 55.73113715427268, 42.68857499666641]
epoch: 48  lr: 0.0005  train_time: 286.8s  val_time: 151.8s  train loss: 2.2156  val loss: 2.0041  val_acc: 58.4256  val_edit: 55.8059 F1s: [51.206066417915366, 45.044714895646024, 33.8061206233542]
epoch: 49  lr: 0.0005  train_time: 251.1s  val_time: 141.9s  train loss: 2.1006  val loss: 2.2150  val_acc: 47.0888  val_edit: 46.7936 F1s: [43.537410448527574, 37.39751859435009, 25.937549991524584]


**************************************************************  Best Acc ***************************************************************

epoch: 40	lr: 0.0005	val_acc: 64.6516	val_edit: 63.5871	F1s: [62.007309294151256, 55.54652099671124, 42.62494440183123]

**************************************************************  Best Edit **************************************************************

epoch: 47	lr: 0.0005	val_acc: 60.2461	val_edit: 64.8336	F1s: [62.51579792123428, 55.73113715427268, 42.68857499666641]

**************************************************************  Best F1 ***************************************************************

epoch: 47	lr: 0.0005	val_acc: 60.2461	val_edit: 64.8336	F1s: [62.51579792123428, 55.73113715427268, 42.68857499666641]

**************************************************************   config  ****************************************************************

tmse_weight 0.15   optimizer:  Adam  scheduler:  None n_classes:  48
kernel_size 15   n_features:  64  in_channel:  2048
Dataset: breakfast	Split: 4
Batch Size: 1	Num in channels: 2048	Num Workers: 4
Dataset: breakfast	Split: 4
train_data:  1136

***************************************************************************************************************************************

All_time: 410.9755min
./result/breakfast/ms-tcn/split4
