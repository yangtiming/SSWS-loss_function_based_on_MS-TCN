nohup: ignoring input
Dataset: breakfast	Split: 4
Batch Size: 1	Num in channels: 2048	Num Workers: 4

------------------------Loading Model------------------------

Multi Stage TCN will be used as a model.
stages: ['dilated', 'dilated', 'dilated', 'dilated']	n_features: 64	n_layers of dilated TCN: 10	kernel_size of ED-TCN: 15
Adam will be used as an optimizer.

---------------------------Start training---------------------------

epoch: 0  lr: 0.0005  train_time: 546.3s  val_time: 233.0s  train loss: 13.1328  val loss: 3.0622  val_acc: 12.8566  val_edit: 23.3592 F1s: [24.86485987370969, 20.830581693090245, 11.601840757033184]
epoch: 1  lr: 0.0005  train_time: 417.9s  val_time: 237.1s  train loss: 10.4290  val loss: 2.5694  val_acc: 20.2768  val_edit: 34.0854 F1s: [36.11232505766801, 27.468182890050848, 15.291789735159048]
epoch: 2  lr: 0.0005  train_time: 343.5s  val_time: 181.8s  train loss: 8.9284  val loss: 2.5055  val_acc: 18.3817  val_edit: 36.0849 F1s: [35.2786260668826, 28.718731376228956, 15.774457571929512]
epoch: 3  lr: 0.0005  train_time: 313.7s  val_time: 187.5s  train loss: 7.9662  val loss: 2.2425  val_acc: 24.5748  val_edit: 39.0663 F1s: [37.92345439656219, 30.986236221050717, 18.268002899279974]
epoch: 4  lr: 0.0005  train_time: 313.4s  val_time: 184.0s  train loss: 7.3480  val loss: 2.0345  val_acc: 27.3723  val_edit: 45.3269 F1s: [42.3420533791351, 35.10984482242335, 21.88583557192444]
epoch: 5  lr: 0.0005  train_time: 316.4s  val_time: 184.9s  train loss: 6.7761  val loss: 1.9247  val_acc: 32.4442  val_edit: 50.6667 F1s: [47.01761346984155, 39.41837763880789, 25.811924656426555]
epoch: 6  lr: 0.0005  train_time: 317.8s  val_time: 183.8s  train loss: 6.1907  val loss: 1.8578  val_acc: 35.0736  val_edit: 52.3470 F1s: [48.22437392257083, 40.03412117649008, 27.428810309475328]
epoch: 7  lr: 0.0005  train_time: 320.0s  val_time: 177.1s  train loss: 6.0660  val loss: 1.8477  val_acc: 35.7587  val_edit: 46.7140 F1s: [43.10743309978069, 36.20936147443644, 24.528920703086897]
epoch: 8  lr: 0.0005  train_time: 314.0s  val_time: 181.2s  train loss: 5.3867  val loss: 1.9598  val_acc: 32.9076  val_edit: 50.3474 F1s: [45.648459287062686, 37.393339833137894, 25.277298877506798]
epoch: 9  lr: 0.0005  train_time: 309.5s  val_time: 180.5s  train loss: 5.2585  val loss: 2.2640  val_acc: 30.3017  val_edit: 43.5222 F1s: [41.00952494207989, 33.74174821153231, 22.117494362962148]
epoch: 10  lr: 0.0005  train_time: 312.6s  val_time: 174.9s  train loss: 5.0595  val loss: 1.6716  val_acc: 41.3271  val_edit: 53.0868 F1s: [50.99226588536674, 43.09587983982682, 29.20408957452522]
epoch: 11  lr: 0.0005  train_time: 312.2s  val_time: 181.2s  train loss: 5.0132  val loss: 1.5861  val_acc: 47.7296  val_edit: 57.5654 F1s: [57.11185362376211, 48.01995821310797, 33.52500323860455]
epoch: 12  lr: 0.0005  train_time: 310.2s  val_time: 184.4s  train loss: 4.6506  val loss: 2.3228  val_acc: 33.5843  val_edit: 52.7031 F1s: [51.20995125088426, 43.16662329513417, 29.753394440559646]
epoch: 13  lr: 0.0005  train_time: 312.9s  val_time: 183.4s  train loss: 4.5603  val loss: 1.6724  val_acc: 45.2106  val_edit: 51.2680 F1s: [47.90256619906626, 41.219938948103504, 30.10304501659545]
epoch: 14  lr: 0.0005  train_time: 319.3s  val_time: 178.3s  train loss: 4.2072  val loss: 1.6029  val_acc: 46.0489  val_edit: 57.5874 F1s: [55.445539639571976, 47.32672775838392, 33.50934602021031]
epoch: 15  lr: 0.0005  train_time: 308.5s  val_time: 183.0s  train loss: 4.1515  val loss: 1.7022  val_acc: 46.9858  val_edit: 60.1604 F1s: [57.75168024790403, 49.1193688758667, 35.52945802568207]
epoch: 16  lr: 0.0005  train_time: 312.9s  val_time: 181.7s  train loss: 4.3997  val loss: 1.7878  val_acc: 48.4301  val_edit: 55.2442 F1s: [54.749206282894804, 46.632570443486756, 32.98878848287332]
epoch: 17  lr: 0.0005  train_time: 312.8s  val_time: 179.0s  train loss: 3.6894  val loss: 1.7660  val_acc: 41.5350  val_edit: 55.3525 F1s: [51.85683339011844, 44.51923662135379, 31.07819544554341]
epoch: 18  lr: 0.0005  train_time: 314.7s  val_time: 179.0s  train loss: 4.0089  val loss: 1.6779  val_acc: 49.5315  val_edit: 58.8778 F1s: [58.11261470302907, 51.235249913356306, 37.45826349003132]
epoch: 19  lr: 0.0005  train_time: 314.1s  val_time: 176.6s  train loss: 3.7234  val loss: 1.8006  val_acc: 46.2125  val_edit: 48.3610 F1s: [48.72128696093293, 41.520404401510696, 29.365154175856528]
epoch: 20  lr: 0.0005  train_time: 319.7s  val_time: 179.4s  train loss: 3.7446  val loss: 2.0959  val_acc: 43.8085  val_edit: 49.5856 F1s: [49.225361828399286, 41.97479376394988, 29.5600034313742]
epoch: 21  lr: 0.0005  train_time: 318.5s  val_time: 180.3s  train loss: 3.6520  val loss: 1.8167  val_acc: 49.7906  val_edit: 59.4244 F1s: [59.71007585761251, 51.80922380494023, 38.087856094417006]
epoch: 22  lr: 0.0005  train_time: 318.5s  val_time: 177.1s  train loss: 3.3546  val loss: 2.3712  val_acc: 46.8233  val_edit: 53.5646 F1s: [53.88600547045305, 45.639027922957446, 31.43350115266407]
epoch: 23  lr: 0.0005  train_time: 310.8s  val_time: 175.7s  train loss: 3.5853  val loss: 1.8621  val_acc: 44.1307  val_edit: 59.7437 F1s: [56.00600112197549, 48.0265931425676, 35.02788014385478]
epoch: 24  lr: 0.0005  train_time: 315.1s  val_time: 182.4s  train loss: 3.1166  val loss: 1.6943  val_acc: 56.3446  val_edit: 57.9113 F1s: [58.78572519026912, 51.584307064853704, 37.048521556322996]
epoch: 25  lr: 0.0005  train_time: 306.0s  val_time: 181.5s  train loss: 3.0753  val loss: 1.7601  val_acc: 48.9432  val_edit: 58.7287 F1s: [57.398998952174416, 51.13447211210806, 38.184831824338445]
epoch: 26  lr: 0.0005  train_time: 310.5s  val_time: 181.5s  train loss: 3.5651  val loss: 1.8243  val_acc: 50.6743  val_edit: 59.8689 F1s: [57.11916614813948, 49.51294853155922, 36.53885526731069]
epoch: 27  lr: 0.0005  train_time: 305.0s  val_time: 189.0s  train loss: 2.9301  val loss: 1.9180  val_acc: 51.6209  val_edit: 60.1256 F1s: [59.24741011168763, 51.31648590318114, 37.13889289603923]
epoch: 28  lr: 0.0005  train_time: 305.6s  val_time: 186.8s  train loss: 3.3141  val loss: 1.7847  val_acc: 54.2602  val_edit: 57.4920 F1s: [58.630072413774755, 50.87661493800437, 36.37155123333289]
epoch: 29  lr: 0.0005  train_time: 307.3s  val_time: 180.3s  train loss: 2.9587  val loss: 2.1012  val_acc: 53.7681  val_edit: 58.8895 F1s: [60.3037810437123, 52.822484285607615, 39.26546773607254]
epoch: 30  lr: 0.0005  train_time: 305.9s  val_time: 184.4s  train loss: 2.7497  val loss: 1.9539  val_acc: 53.2369  val_edit: 63.9807 F1s: [63.19466700129131, 54.83130855604627, 40.703128623739396]
epoch: 31  lr: 0.0005  train_time: 308.0s  val_time: 182.6s  train loss: 3.6443  val loss: 1.8363  val_acc: 49.6630  val_edit: 58.9275 F1s: [57.96322590606911, 50.54740272903964, 37.67816083686876]
epoch: 32  lr: 0.0005  train_time: 310.1s  val_time: 185.7s  train loss: 2.5826  val loss: 2.1328  val_acc: 50.5007  val_edit: 59.2340 F1s: [57.4458633968147, 50.0883733393689, 37.14095840344331]
epoch: 33  lr: 0.0005  train_time: 307.6s  val_time: 184.7s  train loss: 2.7686  val loss: 1.8614  val_acc: 57.5089  val_edit: 61.9885 F1s: [62.69213386646862, 55.05050013787843, 41.963104441787216]
epoch: 34  lr: 0.0005  train_time: 305.7s  val_time: 182.0s  train loss: 2.7704  val loss: 1.9836  val_acc: 57.2843  val_edit: 58.2893 F1s: [57.792063712728314, 50.82528986492558, 37.663446349384486]
epoch: 35  lr: 0.0005  train_time: 301.4s  val_time: 188.1s  train loss: 2.4515  val loss: 1.9276  val_acc: 57.7291  val_edit: 63.0501 F1s: [62.3568232777042, 55.37444442307869, 40.46255014995112]
epoch: 36  lr: 0.0005  train_time: 316.8s  val_time: 188.5s  train loss: 3.2449  val loss: 1.9734  val_acc: 58.1900  val_edit: 58.7279 F1s: [57.45757189222269, 50.054574968535135, 36.51880043246306]
epoch: 37  lr: 0.0005  train_time: 309.6s  val_time: 183.7s  train loss: 2.4845  val loss: 2.1944  val_acc: 52.8701  val_edit: 58.2147 F1s: [58.19070412036679, 51.63369145088475, 38.40853030396337]
epoch: 38  lr: 0.0005  train_time: 313.6s  val_time: 183.5s  train loss: 2.6292  val loss: 2.4502  val_acc: 46.3264  val_edit: 54.4524 F1s: [51.49583699065617, 45.3396646178298, 32.68171019510617]
epoch: 39  lr: 0.0005  train_time: 307.1s  val_time: 185.5s  train loss: 3.0018  val loss: 1.8814  val_acc: 47.7493  val_edit: 55.4299 F1s: [52.826082196266576, 45.65216915278839, 31.620548599428925]
epoch: 40  lr: 0.0005  train_time: 314.5s  val_time: 185.6s  train loss: 2.6408  val loss: 2.0973  val_acc: 55.7417  val_edit: 48.4364 F1s: [50.01543203729074, 43.7995217769224, 31.532361130632598]
epoch: 41  lr: 0.0005  train_time: 310.6s  val_time: 185.4s  train loss: 2.3490  val loss: 1.9808  val_acc: 52.4545  val_edit: 60.2981 F1s: [56.99419173657847, 49.469677027753235, 36.86211248903016]
epoch: 42  lr: 0.0005  train_time: 312.4s  val_time: 177.9s  train loss: 3.2539  val loss: 1.7636  val_acc: 57.5197  val_edit: 50.3573 F1s: [52.1088001673166, 45.36062463893073, 33.11533983180363]
epoch: 43  lr: 0.0005  train_time: 316.7s  val_time: 180.8s  train loss: 2.4103  val loss: 2.0814  val_acc: 50.2402  val_edit: 60.3695 F1s: [55.78203354997928, 48.35519197581183, 35.015131430907886]
epoch: 44  lr: 0.0005  train_time: 323.4s  val_time: 178.3s  train loss: 2.2785  val loss: 1.9909  val_acc: 57.6667  val_edit: 63.0389 F1s: [59.677414521910435, 52.150532801480395, 39.33001998096771]
epoch: 45  lr: 0.0005  train_time: 314.7s  val_time: 177.8s  train loss: 2.3634  val loss: 2.0744  val_acc: 62.0523  val_edit: 62.4444 F1s: [61.272042153958814, 54.64457021381991, 41.28273162483021]
epoch: 46  lr: 0.0005  train_time: 320.7s  val_time: 177.8s  train loss: 2.9082  val loss: 1.8286  val_acc: 59.1530  val_edit: 59.6336 F1s: [56.8152818784052, 50.44585512681289, 37.432626317405905]
epoch: 47  lr: 0.0005  train_time: 294.5s  val_time: 162.9s  train loss: 2.1869  val loss: 2.2288  val_acc: 50.9926  val_edit: 57.6347 F1s: [56.1460071047742, 49.53110340405832, 34.68790485611055]
epoch: 48  lr: 0.0005  train_time: 263.7s  val_time: 139.7s  train loss: 2.0722  val loss: 2.3254  val_acc: 55.8821  val_edit: 54.1569 F1s: [47.39630454019843, 41.677320436416295, 30.988484331903404]
epoch: 49  lr: 0.0005  train_time: 170.9s  val_time: 86.3s  train loss: 2.8426  val loss: 2.1479  val_acc: 57.5516  val_edit: 55.7780 F1s: [54.65427793559605, 48.028890938692086, 35.41795182620516]


**************************************************************  Best Acc ***************************************************************

epoch: 45	lr: 0.0005	val_acc: 62.0523	val_edit: 62.4444	F1s: [61.272042153958814, 54.64457021381991, 41.28273162483021]

**************************************************************  Best Edit **************************************************************

epoch: 30	lr: 0.0005	val_acc: 53.2369	val_edit: 63.9807	F1s: [63.19466700129131, 54.83130855604627, 40.703128623739396]

**************************************************************  Best F1 ***************************************************************

epoch: 30	lr: 0.0005	val_acc: 53.2369	val_edit: 63.9807	F1s: [63.19466700129131, 54.83130855604627, 40.703128623739396]

**************************************************************   config  ****************************************************************

tmse_weight 0.15   optimizer:  Adam  scheduler:  None n_classes:  48
kernel_size 15   n_features:  64  in_channel:  2048
Dataset: breakfast	Split: 4
Batch Size: 1	Num in channels: 2048	Num Workers: 4
Dataset: breakfast	Split: 4
train_data:  1136

***************************************************************************************************************************************

All_time: 413.9240min
./result/breakfast/ms-tcn/split4
