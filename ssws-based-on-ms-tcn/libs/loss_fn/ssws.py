import torch
import torch.nn as nn
import torch.nn.functional as F

import time
import numpy as np
import os
import math

class TMSE_and_SSWS(nn.Module):   # 平滑损失函数

    def __init__(self, threshold=4, ignore_index=255):
        super().__init__()
        self.threshold = threshold
        self.ignore_index = ignore_index
        self.mse = nn.MSELoss(reduction='none')  #这个MSElosss损失函数 这个博客有讲 https://blog.csdn.net/hao5335156/article/details/81029791
    def forward(self, preds, gts):

        total_loss = 0.
        batch_size = preds.shape[0]
        for pred, gt in zip(preds, gts):
            pred = pred[:, torch.where(gt != self.ignore_index)[0]]  # bitch_size 不为1 时 去除补0
            
            
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………## 
         #####################     找出错误帧  （用于有监督部分）     ############################## 
         #####################     找出错误帧  （用于有监督部分）     ##############################
            #举一个很简单的例子 假设 
            #                  pred= [0.1,0.2,0.4,0.8
            #                         0.7,0.3,0.5,0.1
            #                         0.2,0.5,0.1,0.1]C*N
            
            #              pred_one= [ 1 , 2 , 1 , 0 ]    
            #                    gt= [ 1 , 0 , 0 , 1 ]
            
            #              diff_   = [ 0 , 2 , 1 , 1 ]
            #       归一化  diff_   = [ 0 , 1 , 1 , 1 ]
                             #          其中1为错误帧
            ##总体思路: 将C*N矩阵pred变成1*N的一维矩阵pred_one(这个一维矩阵每一帧为此帧的类别种类) , 然后真实值gt与pred_one做差，归一化，找出错误帧 
                     #其中C是种类的类别数， N 代表总帧数
            pred=F.log_softmax(pred,dim=1)
            pred_one=(torch.argmax(pred,dim=0))              #返回指定维度最大值的序号
            diff_=abs(gt-pred_one)                           #找出gt 与pred_one不同的帧数
            diff_[diff_ < 1]  = 0                            #零归一化为0
            diff_[diff_ > 0 ] = 1                            #非零归一化为1
            
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………## 
            
        ######################     设置滑动窗口长度maxx （用于滑动窗口部分）      ############################    
        ######################     设置滑动窗口长度maxx （用于滑动窗口部分）      ############################
            #举一个很简单的例子 假设 gt = torch.tensor([1,1,2,2,2,1,1,1,5,5,5,7,7,7,7,7,6,6,6,6,6,6])
            
            #         diff_gt     =     tensor([0, 1, 0, 0, 1, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])
            #           diff_gt归一化    tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])
            #          diff_gt后面补1    tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])
            #        diff_gt前面面补1 tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])
            #          spilt_dir   =    [[ 0  2  5  8 11 16 22]]
            #        spilt_dir_sum =     [[2 3 3 3 5 6]]
            #此时运行结果找出最小段长度 min(spilt_dir_sum[0])=2,段数len(spilt_dir[0])=7
            
            ##总体思路: 求出 真实值gt 的每一段的断点 位置，然后前后补1(目的是为后续找出最小段的长度)，找出断点的位置具体数值，然后差分，得出每段长度->找出最小段长度和段数
            
            diff_gt=abs(gt[1:]-gt[:-1])                                                     #真时值gt做差找出分段点
            diff_gt[diff_gt < 1]  = 0                                                       #零归一化为0
            diff_gt[diff_gt > 0 ] = 1                                                       #非零归一化为1
            diff_gt_=torch.cat((diff_gt.cuda() ,torch.tensor([1]).cuda()),-1)               #在diff_gt后面补一个1
            diff_gt_normal=torch.cat((torch.tensor([1]).cuda(),diff_gt_.cuda()),-1)         #在diff_gt前面补一个1
            spilt_dir=  (np.argwhere(diff_gt_normal.cpu().detach().numpy()>0).transpose())  #找出每一个非零值(也即是断点的位置)
            spilt_dir_sum=np.diff(spilt_dir)                                                #差分求出最小段的长度
            maxx=int(min(spilt_dir_sum[0])/(len(spilt_dir[0])))                             #min(spilt_dir_sum[0])求出最小段长度；len(spilt_dir[0])->段数
            #这个地方的maxx用于后续设置滑动窗口长度                                     
            
            if maxx<=1:                                                                      #用于防止maxx小于1报错（对于gtea 会出现maxx =0的情况存在）
                maxx=1  
                
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………## 
        ######################     矩阵变换1*N的矩阵变成C*(N-2*maxx）矩阵  （用于有监督部分）    ############################
            #举一个很简单的例子 ：这里的max取1
            #            diff_ =  tensor([ 0,  0 , 0 , 1 , 0 , 0 , 0 , 1 , 1 ])
            
            #                b =  tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],
            #                             [1., 1., 1., 1., 1., 1., 1., 1., 1.],
            #                             [1., 1., 1., 1., 1., 1., 1., 1., 1.],
            #                             [1., 1., 1., 1., 1., 1., 1., 1., 1.]])
            
            #             diff_ = tensor([[0., 0., 0., 1., 0., 0., 0., 1., 1.],
            #                             [0., 0., 0., 1., 0., 0., 0., 1., 1.],
            #                             [0., 0., 0., 1., 0., 0., 0., 1., 1.],
            #                             [0., 0., 0., 1., 0., 0., 0., 1., 1.]])
            
            #             diff_ = tensor([[    0., 0., 1., 0., 0., 0., 1.    ],
            #                                 [0., 0., 1., 0., 0., 0., 1.],
            #                                 [0., 0., 1., 0., 0., 0., 1.],
            #                                 [0., 0., 1., 0., 0., 0., 1.]])
            ##总体思路: 这里的diff_为61行的diff_, 这段的目的在于 将diff_变成一个C*（N-2*maxx）【为什么不是N，因为滑动窗口会损失掉前后各maxx帧】
        ##################################################
            b=torch.ones(pred.shape[0],pred.shape[1]).cuda()       #生成一个和pred一样的C*N的单位矩阵
            diff_=diff_.float()*b                                          # 把diff_变成一个one_hot
            diff_=diff_[:,maxx:-maxx]                       
            
            
            

##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##             
            ######################     核心代码区     ############################
            ######################     滑动窗口部分     ############################
            ######################     核心代码区     ############################
            ######################     滑动窗口部分     ############################
            
        
        
            # 这里先做一个假设，maxx为3
            #假设取第2帧为基准帧
            #这一段主要是第2帧减掉第0帧                         
                                                           #而dim=0代表 列 取 softmax。根据我写的内容 应该是dim=0 更合理
            L_1 = F.log_softmax(pred[:, maxx:], dim=1)    #(删除前maxx帧）截取pred的 后  N-maxx项让pred 变成一个C*(N-maxx)的矩阵
            L_2 = F.log_softmax(pred[:, :-maxx], dim=1)   #(删除后maxx帧)截取pred的 前  N-maxx项让pred 变成一个C*(N-maxx)的矩阵
            loss1 = (self.mse(L_1,L_2)[:,:-maxx]) #｜2-0｜ #两者做平方差后，再 截取 前 N-maxx项让pred 变成一个C*(N-2maxx)的矩阵
            
            
            #这一段主要是第2帧减掉第5帧               
            L_3 = F.log_softmax(pred[:, maxx:-maxx], dim=1) #(删除前maxx帧)(删除后maxx帧)截取pred的 前 后  N-maxx项让pred 变成一个C*(N-2maxx)的矩阵
            L_4 = F.log_softmax(pred[:, 2*maxx:], dim=1)    #(删除前2*maxx帧) 截取pred的   后   N-2maxx项让pred 变成一个C*(N-2maxx)的矩阵
            loss2 = self.mse(L_3,L_4)         #｜2-5｜       #两者做平方差
            
            
            #这一段主要是第5帧减掉第0帧            
            L_5 = F.log_softmax(pred[:, :-2*maxx], dim=1)  #(删除后2*maxx帧)截取pred的 前  N-2maxx项让pred 变成一个C*(N-2maxx)的矩阵
            L_6 = F.log_softmax(pred[:, 2*maxx:], dim=1)   #(删除前2*maxx帧)截取pred的 后  N-2maxx项让pred 变成一个C*(N-2maxx)的矩阵
            loss3 = self.mse(L_5 , L_6)     #｜0-5｜      #两者做平方差
            
            
            a=(loss1>torch.mean(loss1)).float()                        #取torch.mean(loss1)的平均值作为阈值。高于torch.mean(loss1)的值为True,否则为False
            b=(loss2>torch.mean(loss2)).float()                          #取torch.mean(loss2)的平均值作为阈值。高于torch.mean(loss2)的值为True,否则为False
            L_sw=(loss1+loss2+loss3)*a*b                     #将三个loss求和，在True的地方保留原来的值，在False的地方值置为0.
            

            
            ssws_loss=torch.mean( (loss1+loss2+loss3)*a*b + (loss1+loss2+loss3)*a*b*diff_ ) ##有监督部分和无监督部分结合
                                                                                    ##(loss1+loss2+loss3)*a*b为无监督部分
                                                                                    ##(loss1+loss2+loss3)*a*b*diff_有监督部分，错误的地方错惩罚一点

##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##   
            ###############本部分ms-tcn的原本平滑函数##############
            ###############本部分ms-tcn的原本平滑函数##############
                                                                
            loss = self.mse(
                F.log_softmax(pred[:, 1:], dim=1),
                F.log_softmax(pred[:, :-1], dim=1)
            )            
            loss = torch.clamp(loss, min=0, max=self.threshold**2)
            loss = torch.mean(loss)
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##    
########################################################################################################################
##……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………##   
            
            total_loss += (loss+1*ssws_loss)            #我的方法和原ms-tcn论文里面的内容相互结合
        return total_loss / batch_size

